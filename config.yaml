defaults:
  - modulus_default
  - scheduler: tf_exponential_lr
  - optimizer: adam
  - loss: sum
  - _self_

bounds:
  r:
    - [0, 1]
    - [0, 1]
    - [0, 1]
  v:
    - [0, 1]
    - [0, 1]
    - [0, 1]
  t:
    - [0, 1]
    - [0, 1]
    - [0, 1]

geometry:
  grid_resolution: 1000

scheduler:
  decay_rate: 0.95
  decay_steps: 4000

neural_network:
  f_e:
    activations: [GeLU, GeLU, GeLU, GeLU, GeLU, Sigmoid]
    hidden_layers: [16, 32, 64, 32, 10, 1]
  f_p:
    activations: [GeLU, GeLU, GeLU, GeLU, GeLU, Sigmoid]
    hidden_layers: [16, 32, 64, 32, 10, 1]
  f_E:
    activations: [GeLU, GeLU, GeLU, GeLU, GeLU, null]
    hidden_layers: [16, 32, 64, 32, 10, 1]
  f_B:
    activations: [GeLU, GeLU, GeLU, GeLU, GeLU, null]
    hidden_layers: [16, 32, 64, 32, 10, 1]

fno:
  f_e:
  f_p:
  f_E:
  f_B:

training:
  rec_validation_freq: 1000
  rec_inference_freq: 2000
  rec_monitor_freq: 1000
  rec_constraint_freq: 2000
  max_steps: 10000

batch_size:
  TopWall: 1000
  NoSlip: 1000
  Interior: 4000

graph:
  func_arch: true
