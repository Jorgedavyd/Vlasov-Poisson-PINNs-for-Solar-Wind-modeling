\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{cite}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}

\title{Vlasov-Maxwell-Liouville informed Operator Learning for Solar Wind Modeling}
\author{Jorge D. Enciso}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
    Solar wind modeling presents significant challenges due to its complex, quasi-neutral, and collisionless plasma dynamics. Traditional magnetohydrodynamic (MHD) models often fail to accurately capture the system's intricate behavior. This research proposes a novel approach using Statistical Mechanics Informed Neural Networks to develop a more nuanced kinetic description of solar wind phenomena. By integrating Vlasov-Maxwell equations with physics-informed neural network architectures, we develop a computational framework that overcomes limitations of existing analytical and numerical models. The proposed methodology leverages data from space observatories including DSCOVR, ACE, and WIND, establishing boundary conditions from the L1-Lagrange point to Earth's orbit. Our approach directly embeds physical constraints into the neural network's learning process, enabling a more interpretable and computationally efficient model. The research demonstrates how physics-informed machine learning can provide insights into the complex dynamics of solar wind, moving beyond traditional temperature distributions and computational constraints.By combining statistical mechanics, kinetic theory, and advanced machine learning techniques, this work offers a promising new paradigm for understanding and predicting solar wind behavior, with potential implications for space weather prediction and fundamental plasma physics research.
\end{abstract}

\tableofcontents
\newpage

\section{Introduction}

The solar wind phenomena is a vastly known event that arises from ionized solar outbursts into the stellar medium, commonly stimulating planets' magnetosphere. \cite{Gosling2007} The first record of a related event dates from 1859, popularly named "The Carrington event" after the English astronomer Richard Carrington (1826, 1875), who settled the intuition upon solar flares and geomagnetic fluctuations. Individuals from all around the glove sighted the Northern Lights, subtle variations in atmosphere's hues' tones, caused by ionized particles flowing through the ionosphere.

Several attempts to model Solar Wind's dynamics can be empirically pinpointed: analytical methods \cite{BLUME202396}, numerical modelling \cite{10.3389/fspas.2023.1105797, windmodelling1, Gombosi_2018}, and machine learning approaches \cite{comp_2, comp_3, guastavino2024forecastinggeoffectiveeventssolar, sabbatini2023solarwindspeedestimate, https://doi.org/10.1029/2023SW003561}. All of them with different purposes.

The solar wind presents unique challenges due to its deviation from classical assumptions. It is a quasy-neutral, colisionless, and empirically quasy-isotropic ionized plasma. \cite{Verscharen2022} Therefore, the adaptation of plasma dynamics frameworks for solar wind modeling must be carefully analyzed. For instance, the ideal magnetohydrodynamics (Ideal MHD) model is used under the assumption of an ionized plasma that converges to the Boltzmann distribution (Maxwell-Boltzmann distribution for temperatures), but it's an empirically unfeasible distribution as it misrepresents historical observations on solar wind's behavior.

On the other hand, using numerical approximators to model plasma dynamics ends up being computationally expensive. Take the Ideal MHD as an example:

\begin{equation}
    \nabla \cdot \textbf{E} = \frac{\sigma}{\epsilon_0}
\end{equation}
\begin{equation}
    \nabla \cdot \textbf{B} = 0
\end{equation}
\begin{equation}
    \frac{\partial \rho}{\partial t} + \nabla \cdot (\rho \textbf{v}) = 0
\end{equation}
\begin{equation}
    \frac{d}{dt} \left( \frac{p}{\rho^\gamma} \right)= 0
\end{equation}
\begin{equation}
    \textbf{E} + \textbf{v} \times \textbf{B} = 0
\end{equation}
\begin{equation}
    \frac{\partial \textbf{B}}{\partial t} = \nabla \times (\textbf{v} \times \textbf{B})
\end{equation}
\begin{equation}
    \textbf{J} \times \textbf{B} = \frac{(\textbf{B}\cdot \nabla) \textbf{B}}{\mu_0} - \nabla \left(\frac{B^2}{2\mu_0}\right)
\end{equation}

where $\mathbf{E}$ is the electric field, $\sigma$ the charge density, $\mathbf{B}$ the magnetic field, $p$ the pressure, $\rho$ the mass density, $v$ the general velocity, and $\gamma $ the adiabatic index.

Nevertheless, recent research on machine learning computational methods enable novel approaches towards PDE solving and operator learning. These architectures, named Physics Informed Neural Networks, allow learnable solutions for less computational price thanks to modern hardware optimizations and shallower representations with universal approximators (in this case, neural networks). Hence, the scope of this research is to adhere these concepts on a deeper basis, proposing a new methodology within numerical statistical mechanics for solar wind modeling.

To this date, the literature doesn't embark on statistical mechanics informed neural networks for probability density function learning on a solar wind scheme. Therefore, the objective of this work is to leverage statistical mechanics theory and physics informed neural networks to propose a new numerical approximation for the kinetic behavior of the solar wind.

\section{Related Work}

\subsection{Full compressible 3D MHD simulation of solar wind}
On the matter of Solar Wind modeling, several numerical approaches are developed to understand near-Sun physics given certain boundary conditions. This research \cite{windmodelling1} creates a magnetohydrodynamical modeling scheme to understand these interactions under certain restrictions that approach the conditions from the transition from the photosphere to the heliospheric distance of 27 solar radii. In this case, the current work is seeking more general descriptors of the Solar Wind, with real L1-Lagrange data as a cornerstone for the modeling scheme to finally get a kinetic description of the phenomena.

\subsection{Physics informed Neural Networks applied to the description of wave-particle resonance in kinetic simulations of fusion plasmas}

Regarding Vlasov-Maxwell modeling with physics informed neural networks, this paper constitutes the first machine learning approach intersecting both this theory and the novel PDE solving scheme proposed by PINNs. \cite{kumar2023physicsinformedneuralnetworks} It employs the model onto fusion modeling within the colisionless constraint. It constitutes an important basis for the present paper, offering a perspective of the efficiency and satisfaction offered by using these methods on alternative fields.

They use the VOICE code to numerically solve the Vlasov-Poisson system of equations, and use this data as ground truth for the Physics Informed Neural Network. Given that it's an Integro-Differential equation, they advocate for the usage of I-PINN (integrable-PINN) \cite{} which uses the fundamental theorem of Calculus to approach the integrable part. This integration approach will be used by the present work as well to compute several constraints tied to statistical mechanics.

\subsection{Physics-Informed Neural Networks for Solar Wind Prediction}

This research \cite{johnson2022physics} is regarded as one of the first physics informed approaches towards geoeffective solar wind prediction. It uses L1-Lagrange spacecraft readings and enforces the Ohm's Law for an ideal plasma, following the Ideal MHD model for space plasma. It uses a wide variety of model architectures (GRU, LSTM, 2D CNN ResNets, etc.) to test their effectiveness under this task.

\section{Physics Informed Machine Learning}
Usually, solely data-driven methods require huge volumes of high-quality data with black box algorithms that prevent interpretability, a key factor for physical modeling. The most recent research advocate for physical informed virtual loss function terms \cite{}, leading off to the vastly known field of Physics Informed Neural Networks.

These models were intended to be numerical solvers for Partial Differential Equations that embed physical constraints in the loss function. It further extended to all fields that required PDE solving as a computationally efficient alternative to other solvers.

\subsection{Physics informed Neural Networks}
The representational power of the neural network enables learning complex non-linear functions in a given linear space. This representational power, coupled with suitable constraints, can lead to the major description of unknown phenomena. In the case of Physics informed Machine Learning, we can define a self-supervised learning procedure (governing equations), and some initial conditions, such that:

\begin{align*}
    \mathcal{L}_{total} = \mathcal{L}_{PDE} + \mathcal{L}_{Boundary}
\end{align*}

This setup lays down the foundations of Physics informed Machine Learning, where instead of relying on data-driven supervised learning, we embed governing partial differential equations that serve as a self-sufficient constraints for the training procedure, and the boundary condition defined in $\mathcal{L}_{initial}$ as a pivot.

Incredibly, this framework challenges the popular believe of the reliance of deep learning architectures to high volumes of data to give meaningful results. Instead, we can rely on Dirichlet or Newmann boundary conditions to fulfill the initial conditions.

\subsection{Approximating functionals with PINNs}
The most recent research, using PINNs, develop a way to not just learn functions, but to learn functional operators: Lagrangian, Hamiltonian, etc. \cite{cranmer2020lagrangianneuralnetworks, greydanus2019hamiltonianneuralnetworks} The first research creates a neural network that takes the general coordinates $q$ and $\dot q$ as input, to provide the lagrangian as an output, taking advantage of the auto-differentiation capabilities of modern machine learning frameworks to enforce the euler-lagrange equation with gradient-descent methods:

\begin{equation}
    \frac{d}{dt} \frac{\partial \mathcal{L}}{\partial \dot q} - \frac{\partial \mathcal{L}}{\partial q} = 0
\end{equation}

The current work extrapolates this concept to the field of density functions, embedding it into the training phase by ensuring the normalized nature of a density function, the second law of thermodynamics, and systems governed by kinetic theory following Liouville's theorem.

This model establishes a cornerstone for solar wind modeling, bonding the kinetic model distribution to the results of the electromagnetic model, extending the training space from     L1 Lagrange to a whole grid interval from L1 to the Earth.

\subsection{Physics informed Neural Operators}

Traditional Physics informed Machine Learning relies on the usage of some fixed input grid to learn a representation of the data utterly dependant on resolution. To solve this problem, a discretization-invariant operator should be embedded in the training process, and this is pragmatically solved with the operators/functionals (mappings between function spaces) learning.

Such a general operator can be defined as a differential or integral operator. A useful way to embed operators, and take advantage of computational shortcuts at inference time, is using some kernel integral operator $\mathcal{G}$:

\begin{align*}
    \mathcal{G}[f(x); \kappa] = \int_\Omega \kappa(x, t) f(t) d(\alpha(t))
\end{align*}

for some region $\Omega$, defined kernel $k$, differentiable real valued functions $f$ and $\alpha$ such that the Riemann definition is fulfilled.

One of such operators could be the convolution, since it is defined as follows:

\begin{align*}
    \mathcal{G}[ f(x); \kappa ] = (\kappa * f)(t) = \int_{-\infty}^\infty \kappa(\tau)f(t-\tau)\tau'
\end{align*}

The original paper presenting PINOs included 3 integral operators suitable for discretization-invariant operator learning, analogous to wavelet transforms, fourier transforms, and laplace transforms \cite{li2023physicsinformedneuraloperatorlearning}.

This can be thought as a Functional analysis constraint model architecture, that pragmatically lands as a primordial Physics informed Machine Learning architecture that enables multi-resolution inference.

\subsubsection{Fourier Neural Operator}

To derive the definition of the Fourier Neural Operator, we must attend to the definition of the convolutional operator defined, and how it can be optimized under the convolution theorem.

As it is vastly known, we can demonstrate that the following is true (Look at the appendix):

\begin{align*}
    \mathcal{G}[ f(x); \kappa ] = (\kappa * f)(t) = \int_{-\infty}^\infty \kappa(\tau)f(t-\tau)\tau' = \mathcal{F}^{-1}[\mathcal{F}[f] \cdot \mathcal{F}[\kappa]]
\end{align*}

So, we can define some frequency parameter space $\theta_\omega$, for some $R \in \theta_\omega$ such that $R = \mathcal{F}[\kappa]$, we get the following:

\begin{align*}
    \mathcal{G}[ f(x); R \in \theta_\omega] = \mathcal{F}^{-1}[\mathcal{F}[f] \cdot R]
\end{align*}

Here, we defined $\mathcal{G}$ as some operators with learnable parameters $R$ within the linear space $\theta_\omega$. This is the FNO, as it appropiately extrapolates some pre-defined operator to the frequency domain for computational ease.


\section{Kinetic models and general constraints}
In order to fully assume the magnetohydrodynamical model, interactions must converge to the Maxwell-Boltzmann thermodynamical distribution for certain conditions, implying a high colisionality degree, contrary to empirical observations of the Solar Wind. Therefore, the utilization of such a constrictive framework must be carefully evaluated before setting it up as a modeling constraints.

Moreover, trying to answer quasy-isotropic behavior on near-earth L1-Lagrange point, different research papers assume the bi-Maxwellian or Kappa temperature distributions \cite{Stansby_2018, Nicolaou_2018, Zouganelis_2004}, carrying out the Maxwell-Boltzmann thermodynamical distribution. Nevertheless, this assumption seems overly ideal. As a possible option, the inclusion of numerical distributions could bring insights into the interactions between plasma ionized particles.

That's why, this work departs from that approach, seeking the description of Solar Wind kinetic model without magnetohydrodynamical systems nor temperature distributions, fully describing these interactions with a physics informed data-driven structure as a substitution of empirical assumptions.

\subsection{Boltzmann-Vlasov Equations}

In the context of kinetic plasma theory, various statistical descriptors are applied, provided they make appropriate assumptions about particle interactions. Boltzmann's equation is often used to describe the evolution of the particle distribution function, $f_\alpha(\mathbf{r}, \mathbf{v}, t)$, by considering the effects of particle collisions:

\begin{equation}
    \frac{\partial f_\alpha}{\partial t} + \mathbf{v} \cdot \nabla_{\mathbf{r}} f_\alpha + \mathbf{a} \cdot \nabla_{\mathbf{v}} f_\alpha = \left(\frac{\delta f}{\delta t}\right)_{\mathrm{col}}
\end{equation}

The left side of the equation describes the rate of change of $f_\alpha$ due to the particles' motion and external forces, while the term $\left( \frac{\delta f_\alpha}{\delta t} \right)_{col}$on the right side represents the change in $f_\alpha$ due to collisions.

This formulation is essential for systems where collisions are frequent and lead to a Maxwell-Boltzmann distribution under equilibrium conditions. However, for plasmas like the solar wind, where collisions are rare, a collisionless description is more appropriate.

The Vlasov equation is the collisionless analog of the Boltzmann equation, making it a more suitable kinetic model for the solar wind. It describes the evolution of the particle distribution function in a collisionless plasma, where interactions are primarily governed by external forces rather than particle collisions:

\begin{equation} \frac{\partial f_\alpha}{\partial t} + \mathbf{v} \cdot \nabla_{\mathbf{r}} f_\alpha + \frac{\mathbf{F}}{m} \cdot \nabla_{\mathbf{v}} f_\alpha = 0 \end{equation}

In this context, the system is typically dominated by electromagnetic interactions. Thus, substituting with the Lorentz force is the ideal case:

\begin{equation}
    \frac{\partial f_\alpha}{\partial t} + \mathbf{v} \cdot \nabla_{\mathbf{r}} f_\alpha + \frac{q}{m} (\mathbf{E} + \mathbf{v} \times \mathbf{B}) \cdot \nabla_{\mathbf{v}} f_\alpha = 0
\end{equation}

This form of Vlasov equation is widely used to capture the dynamics of collisionless plasmas, as it allows us to account for anisotropies in the solar wind, such as those arising from velocity space instabilities and non-thermal distributions of particle energies \cite{Verscharen_2016}. Unlike ideal MHD models, which assume that the Lorentz force term can be neglected, the Vlasov equation includes this term to account for deviations from isotropy and Maxwellian distributions in the solar wind, requiring numerical approximation techniques for practical solutions \cite{grandin2023hybridvlasovsimulationsoftxray}.

\subsection{Entropy}

According to the second law of thermodynamics, the entropy of a closed system cannot decrease over time; it may increase or remain constant. In the kinetic framework, the entropy production rate is given by the time derivative of the entropy functional:

\begin{equation}
    0 \leq \frac{\mathrm{d}}{\mathrm{dt}} \int f_\alpha \log f_\alpha d^3\mathbf{r} d^3\mathbf{v}
\end{equation}

where $\f_\alpha \log f_\alpha$ represents the local entropy density in phase space. In a collisionless plasma, this entropy can remain constant if the system maintains phase-space coherence, but under realistic conditions, such as wave-particle interactions, entropy may increase as energy cascades through different scales. This formalism highlights how irreversible processes in the plasma lead to an increase in degrees of freedom, a key consideration for accurately modeling solar wind behavior.

For this case particularly, the collisionless nature of the plasma and Liouville's theorem assumes no change in entropy, if we use Leibniz method and compute the time derivative of the integrant:

\begin{equation}
    \frac{d}{dt} f_\alpha \log f_\alpha = \frac{df_\alpha}{dt} \log f_\alpha + \frac{f_\alpha}{f_\alpha}\frac{df_\alpha}{dt} = 0 + 0 = 0
\end{equation}

\subsection{Maxwell's equations}

To complete the description of the solar wind, the Maxwell's electrodynamic formulation provides our final description to well-round the kinetic analysis of the plasma:

\begin{align*}
    \nabla \cdot E &= \frac{\sum_{\alpha} \rho_\alpha}{\epsilon_0} \\
    \nabla \cdot B &= 0 \\
    \nabla \times E &= - \frac{\partial B}{\partial t} \\
    \nabla \times B &= \mu_0 \left(J + \epsilon_0 \frac{\partial E}{\partial t}\right)
\end{align*}

\subsection{Conservation of energy}
Another important property can be provided by conservation of energy in solar wind. In this case, we want to account for the bulk motion of the particles; the interaction energy, this is the work of the fields; and the conservation of the pointing vector $S$ describing the energy transport throughout space and time.

\begin{align*}
    \frac{\partial}{\partial t} \left( \frac{1}{2} m_\alpha f_v^2 + m_\alpha q_\alpha f_E \cdot f_v\right) + \nabla \cdot \textbf{S} = 0
\end{align*}

\subsection{Complete dynamical system}
For some general universal approximator $f_n$, also interpreted as dynamical descriptor in this context, there is a parameter space $\theta_n$ tied to the governing constraints ($g_i$) for its respective phenomena:

\begin{align*}
    f_n(\mathbf{r}, \mathbf{v}, t; \theta_n): \nabla f_n = \sum_{i} \lambda_i \nabla g_i
\end{align*}

For this sake, we employ a Statistical mechanics informed Vlasov-Maxwell model that fully describes the known, ideal behavior of the solar wind:

\begin{align*}
    \frac{\partial f_\alpha}{\partial t} + f_v \cdot \nabla_{\mathbf{r}} f_\alpha &+ \frac{q_\alpha ( f_E + f_v \times f_B)}{m_\alpha} \cdot \nabla_{\mathbf{v}} f_\alpha = 0 \text{ (Vlasov)}\\
\frac{d}{dt} \int &f_\alpha \log f_\alpha d^3r d^3(f_v) = 0 \text{ (Liouville)}\\
\nabla \cdot f_E &= 0 \text{ (Quasineutrality)}\\
\nabla \cdot f_B &= 0 \\
\nabla \times f_E &= - \frac{\partial f_B}{\partial t} \\
\frac{\partial}{\partial t} \left( \frac{1}{2} m_\alpha f_v^2 + m_\alpha q_\alpha f_E \cdot f_v \right) &+ \nabla \cdot \textbf{S} = 0 \text{ (Conservation)} \\
\end{align*}

$f_\alpha$ is the kinetic description of each specie, $f_E$ is the electric field descriptor, $f_B$ is the magnetic field descriptor, $f_v$ is the velocity descriptor, $J$ is the current density (derived from $f_\alpha$), and $\rho_\alpha$ is the charge density (derived from $f_\alpha$).

The cross-dependance between both models for the backpropagation process is crucial; thus, a backbone architecture for the electromagnetic descriptor engineered to exploit the backpropagation of the kinetic model can lead to faster convergence in addition to the Maxwell constraints.

A cross-training process that relies on the bonds between the Maxwell and Vlasov PDE criterions will be critical; the PDE loss contribution to the training process will be gradually incremented through epochs, and inversely correlated with the distances from the boundary contour delimiting the training space to minimize the error margin of non-representative results in the middle of the training process.

This structure provides an optimization process overtly relying on PDE residuals; therefore, an appropiate set of boundary conditions is crucial for the convergence of the physics informed neural networks.

\subsubsection{Scope and boundary conditions}
The current work utilizes data from L1-Lagrange readings; accordingly, the phase space is constrained to fit predictions up to the Earth's position. As so, we define our parameter space:

\begin{align*}
    r_x &\in \left[0, 230R_E\right] \\
    v_x &\in \left[0, 1200\right]\frac{km}{s} \\
    t &\in \left[0, 7200]s \\
\end{align*}

For the kinetic and electromagnetic model, we define the following Dirichlet boundary conditions:

\begin{align*}
    f_p(r_0, v, t) = n_{L1}(t) \left( \frac{m_p}{2 \pi k_B T_{L1}(t)}\right)^{\frac{3}{2}}& \exp\left({-\frac{m_p | v_{L1}(t) - u_{L1}(t) |^2}{2k_BT_{L1}(t)}}  \right) \\
    f_p\left(r, v_{min}, t) &= 0 \\
    f_p\left(r, v, t_0) &= 0 \\
    f_e(r_0, v, t) = n_{L1}(t) \left( \frac{m_e}{2 \pi k_B T_{L1, \perp}(t)}\right)&\left( \frac{m_e}{2 \pi k_B T_{L1, \parallel}(t)}\right)^{\frac{3}{2}} \exp\left(-\frac{m_ev_{L1, \perp}^2}{2k_BT_{L1, \perp}} - \frac{m_e (v_{L1, \parallel} - u_{L1, \parallel})^2}{2k_BT_{L1, \parallel}}\right) \\
    f_e\left(r, v_{min}, t) &= 0 \\
    f_e\left(r, v, t_0) &= 0 \\
    f_E\left(r_0, t\right) &= E_{L1}(t) \\
    f_E\left(r, t_0\right) &= 0 \\
    f_B\left(r_0, t\right) &= B_{L1}(t) \\
    f_B\left(r, t_0\right) &= 0 \\
    f_v\left(r_0, t\right) &= V_{L1}(t) \\
    f_v\left(r, t_0\right) &= 0 \\
\end{align*}


\section{Results and discussion}

\subsection{Neural Network 3D-Simulations}

\subsection{Fourier Neural Operator 3D-Simulations}

\section{Conclusion}
This work demonstrated the effectiveness of Physics-Informed Neural Networks to model Solar Wind's behavior tied to diverse statistical mechanics formulations that lead to Vlasov's kinetic model. By embedding the governing physical equations directly into the loss function, we can efficiently train a neural network to approximate the solution of this complex, non-linear system.

\section{Appendix}


\begin{definition}[Riemann-Stieljes integral]
    Given some partition $P = \{x_0, x_1, \ldots, x_n\}$ in $[a, b]$, we define $t_k \in [x_{k-1}, x_k]$ such that a partial sum of the form
    \begin{align*}
        S(P, f, \alpha) = \sum_{k = 1}^n f(t_k) \Delta \alpha_k
    \end{align*}
    is termed sum of Riemann-Stieljes of $f$ with respect to $\alpha$. $f$ is defined as Riemann-integrable with respect to $\alpha$ if $\exists N \in \mathbb{N}$ such that for every $\varepsilon > 0$ there is some $P_\varepsilon \subset [a, b]$ that encapsulates other partition $P$, and for every election of points $t_k \in [x_{k-1}, x_k]$, $\mid S(P, f, \alpha) - N \mid < \varepsilon$.
\end{definition}

\begin{definition}[Fourier Series]
    The Fourier series of a function $f$, periodic under the interval $\Omega : [0, T]$, is given by the following series:
    \begin{align*}
        f(x) = \sum_{n = -\infty}^\infty c_n \exp \left( i 2\pi \frac{n}{T} x\right)
    \end{align*}

    where $c_n$ is some complex number defined within the symmetric consideration $\overline{c_k} = c_{-k}$ $ \forall k \in \mathbb{N}$ that is derived as follows:
    \begin{align*}
        f(x) &= \sum_{n = -\infty}^\infty c_n  e^{i 2\pi \frac{n}{T} x} \\
        f(x) &= c_m e^{i 2\pi \frac{m}{T} x} + \sum_{n \neq m} c_n e^{i 2\pi \frac{n}{T} x} \\
        c_m e^{i 2\pi \frac{m}{T} x} &= f(x) - \sum_{n \neq m} c_n e^{i 2\pi \frac{n}{T} x} \\
        c_m &= e^{- i 2\pi \frac{m}{T} x} f(x) - e^{-i 2\pi \frac{m}{T} x} \sum_{n \neq m} c_n e^{i 2\pi \frac{n}{T} x} \\
        \int_0^T c_m dx &= \int_0^Te^{- i 2\pi \frac{m}{T} x} f(x)dx  - \int_0^T \sum_{n \neq m} c_n e^{i 2\pi \frac{n - m}{T} x} dx\\
        c_m &= \frac{1}{T} \int_0^Te^{- i 2\pi \frac{m}{T} x} f(x)dx
    \end{align*}

From this definition we can derive an interpretation of the Fourier Series as a reflection of an infinite orthogonal base with respect of the function.
\end{definition}

\begin{definition}[Fourier Transform]
    Let $f$ be a non-periodic function. We can define the Fourier transform of $f$ as an extreme case of periodicity, interpreting it as tending to infinity:
    \begin{align*}
        \mathcal{F}[f] = \int_{-\infty}^{\infty} f(x) e^{- i 2 \pi s x} dx
    \end{align*}
\end{definition}

\begin{theorem} [Convolution Theorem]
    Given the following definition of some convolution under some domain $\Omega$ of some differentiable real valued functions $f$ and $g$:
    \begin{align*}
        h(x) = \int_{\Omega} f(t) g(x - t) dt
    \end{align*}
Given that $h \in L^1\left( \mathbb{R}^n\right)$ (the demonstration is out of scope), we can assert the following:
\begin{align*}
    \mathcal{F}[h] = \int_{\Omega} \left(\int_{\Omega} f(t) g(x - t) dt \right)e^{-i 2 \pi s x} dx
\end{align*}
Using the Fubini theorem to interchange the order of integration, we have:
\begin{align*}
    \mathcal{F}[h] = \int_{\Omega} \left(\int_{\Omega} f(t) g(x - t) e^{-i 2 \pi s x} dx\right) dt
\end{align*}
and finally, we make the following substitution $y = x - t$, with its respective jacobian being 1:
\begin{align*}
    \mathcal{F}[h] &= \int_{\Omega} \left(\int_{\Omega} f(t) g(y) e^{-i 2 \pi s (y + t)} dy\right) dt \\
    \mathcal{F}[h] &= \left(\int_{\Omega} f(t) e^{-i 2 \pi s t} dt \right) \left(\int_{\Omega} g(y) e^{-i 2 \pi s y} dy \right) \\
    \mathcal{F}[h] &= \mathcal{F}[f] \cdot \mathcal{F}[g]
\end{align*}
\end{theorem}

\begin{definition}[Boltzmann's equation]
    Given a probability density function $f(\mathbf{r}, \mathbf{v}, t)$ defining the probability of finding a particle in the phase space at the volume $d^3\mathbf{r}d^3\mathbf{v}$, Boltzmann's general statement is as follows:
    \begin{align*}
        \frac{\mathrm{df}}{\mathrm{dt}} = \left( \frac{\partial f}{\partial t} \right)_{force} + \left( \frac{\partial f}{\partial t} \right)_{coll}+ \left( \frac{\partial f}{\partial t} \right)_{diff}
    \end{align*}
    the $force$ term being the change due to external interference, $diff$ the diffusion of the particles, and $coll$ the forces due to collisions.

    Now, given a fixed probability at time $t$ of some particles to be at $r$ and velocity $v$ in the volume $d^3\mathbf{r}d^3\mathbf{v}$, let's consider a shift due to a force $\mathbf{F}$ such that at time $t + \Delta t$, the particle is at $\mathbf{r} + \Delta \mathbf{r}= \mathbf{r} + \Delta t \mathbf{v}$ with velocity $\mathbf{v} + \Delta \mathbf{v} = \mathbf{v} + \frac{\mathbf{F}}{m} \Delta t$, and given Liouville's theorem, the phase space of probabilities doesn't change over time:
    \begin{align*}
        f\left(\mathbf{r} + \mathbf{v} \Delta t, \mathbf{v} + \frac{\mathbf{F}}{m} \Delta t, t + \Delta t\right) d^3\mathbf{r} d^3 \mathbf{v} = f(\mathbf{r}, \mathbf{v}, t) d^3\mathbf{r} d^3 \mathbf{v}
    \end{align*}
    But under collisions, we have to account for the change of the probability in the phase space due to the $\left( \frac{\partial f}{\partial t} \right)_{coll}$ term:
    \begin{align*}
        dN_{coll} &= \left( \frac{\partial f}{\partial t} \right)_{coll} \Delta t d^3\mathbf{r} d^3 \mathbf{v} \\
        dN_{coll} &=  f\left(\mathbf{r} + \mathbf{v} \Delta t, \mathbf{v} + \frac{\mathbf{F}}{m} \Delta t, t + \Delta t\right) d^3\mathbf{r} d^3 \mathbf{v} - f(\mathbf{r}, \mathbf{v}, t) d^3\mathbf{r} d^3 \mathbf{v} = \Delta f d^3\mathbf{r} d^3\mathbf{v} \\
        \frac{dN_{coll}}{d^3\mathbf{r}d^3\mathbf{v}} &= \frac{\Delta f}{\Delta t} \to^{\Delta f \to 0}_{\Delta t \to 0} \to \left(\frac{\partial f}{\partial t} \right)_{coll} = \frac{df}{dt}
    \end{align*}

Finally, if we expand upon the total differential of $f$ we get the following:
\begin{align*}
    df &= \frac{\partial f}{\partial t} dt + \frac{\partial f}{\partial x} dx + \frac{\partial f}{\partial y} dy + \frac{\partial f}{\partial z} dz + \frac{\partial f}{\partial v_x} dv_x + \frac{\partial f}{\partial v_y} dv_y + \frac{\partial f}{\partial v_z} dv_z \\
    df &= \frac{\partial f}{\partial t} dt + \nabla f \cdot d\mathbf{r} + \nabla_{\mathbf{v}} f \cdot d\mathbf{v} \\
    \frac{df}{dt} &= \frac{\partial f}{\partial t} + \nabla f \cdot \frac{d\mathbf{r}}{dt} + \nabla_{\mathbf{v}} f \cdot \frac{d\mathbf{v}}{dt} = \frac{\partial f}{\partial t} + \nabla f \cdot \mathbf{v}+ \nabla_{\mathbf{v}} f \cdot \frac{\mathbf{F}}{m}
\end{align*}

Getting our two conclusions with collisionless interaction of particles (Vlasov Equation) and collisions (Boltzmann Equation):
\begin{align*}
    \frac{\partial f}{\partial t} + \nabla f \cdot \mathbf{v}+ \nabla_{\mathbf{v}} f \cdot \frac{\mathbf{F}}{m} &= 0 \\
    \frac{\partial f}{\partial t} + \nabla f \cdot \mathbf{v}+ \nabla_{\mathbf{v}} f \cdot \frac{\mathbf{F}}{m} &= \left( \frac{\partial f}{\partial t} \right)_{coll} \\
.\end{align*}

\end{definition}

\begin{definition}[Lagrange multipliers]
    Given a real valued function $f: \mathbb{R}^n \to \mathbb{R}$, let's say we want to find the local extrema of this function tied to some constraints $g_i: \mathbb{R}^n \to \mathbb{R}$. We define Lagrange Multipliers $\lambda_i$, real valued scalars, as follows:
    \begin{align*}
        \nabla f = \sum_{ i } \lambda_i \nabla g_i
    \end{align*}
    this definition comes from the colinearity of the gradient of constraints with respect to the function $f$. We can also create a function named \mathbf{Lagrangian function} to make the surrogate:
    \begin{align*}
        \mathcal{L} = f + \mathbf{\lambda} \cdot \mathbf{g}
    \end{align*}
    such that the following describes the extrema under defined constraints:
    \begin{align*}
        \nabla \mathcal{L} = 0
    \end{align*}
\end{definition}

\bibliographystyle{plain}

\bibliography{references}
\end{document}
